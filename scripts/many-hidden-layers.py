"""
Same as one-hidden-layer.py, but with a variable number hidden layers
that can be specified via the CLI.

For example, running
$ python -m scripts.many-hidden-layers --n_hidden 32 16 8
will create a neural network with three hidden layers, the first with 32
neurons, the second with 16 neurons and the third with 8 neurons.
By default, the network has two layers with 32 and 16 neurons, respectively.

TODO: Why so much slower than one-hidden-layer.py when using the same
      number of neurons and epochs?
"""

from src.helpers.mnist import load_minst_dataset_from_data_folder
import numpy as np
import argparse
from src.helpers.activations import ReLU, ReLU_derivative, softmax
from src.helpers.loss_functions import cross_entropy_loss as loss
from src.helpers.misc import get_predictions, get_accuracy, one_hot_encode

# PARSE CLI ARGUMENTS
parser = argparse.ArgumentParser(description="Train a neural network on the MNIST dataset.")
parser.add_argument(
    "--n_hidden",
    type=int,
    nargs="+",
    default=[32, 16],
    help="Number of neurons in each hidden layer",
)
parser.add_argument(
    "--epochs",
    type=int,
    default=500,
    help="Number of times the training set is passed through the network",
)
parser.add_argument(
    "--learning_rate",
    type=float,
    default=0.2,
    help="How much the weights are updated at each iteration",
)
parser.add_argument(
    "--initial_params",
    type=str,
    default="he",
    help="Initialization method for the weights",
    choices=["he", "gaussian", "samson"],
)
args = parser.parse_args()

# HYPERPARAMETERS
n_hidden = args.n_hidden
epochs = args.epochs
learning_rate = args.learning_rate

# FIXED PARAMETERS
n_neurons_input = 28 * 28  # Number of pixels in each image, also dimension of input layer
n_neurons_output = 10  # Number of possible outputs (0-9 digits), same as output layer
mnist_sample_size = 60_000  # Number of samples in the MNIST training set

# DYNAMIC PARAMETERS
n_layers = 1 + len(n_hidden) + 1  # Number of layers in the network
n_neurons = [n_neurons_input] + n_hidden + [n_neurons_output]  # Number of neurons in each layer

# Throw error if any hidden layer has non-positive number of neurons
if any(n <= 0 for n in n_hidden):
    raise ValueError("Number of neurons in hidden layers must be positive")

# Print the configuration
print("Configuration:")
print(f" - n hidden layers: {len(n_hidden)}")
print(f" - n neurons in hidden layers: {n_hidden}")
print(f" - n total layers including input and output: {n_layers}")
print(f" - n neurons in all layers: {n_neurons}")
print(f" - epochs: {epochs}")
print(f" - learning rate: {learning_rate}")
print(f" - initial parameters algorithm: {args.initial_params}")


def init_params():
    """
    Initialize weights and biases for the neural network.
    """
    W = [None] * (n_layers - 1)
    b = [None] * (n_layers - 1)

    # Initialize weights and biases:
    # - W[i] connects layer i to layer i+1 and has shape (n_neurons[i+1], n_neurons[i])
    # - b[i] is a column vector with n_neurons[i+1] elements
    if args.initial_params == "samson":
        for i in range(n_layers - 1):
            W[i] = np.random.rand(n_neurons[i + 1], n_neurons[i]) - 0.5
            b[i] = np.random.rand(n_neurons[i + 1], 1) - 0.5
    elif args.initial_params == "he":
        for i in range(n_layers - 1):
            W[i] = np.random.randn(n_neurons[i + 1], n_neurons[i]) * np.sqrt(2.0 / n_neurons[i])
            b[i] = np.zeros((n_neurons[i + 1], 1))
    elif args.initial_params == "gaussian":
        for i in range(n_layers - 1):
            W[i] = np.random.randn(n_neurons[i + 1], n_neurons[i]) * 0.01
            b[i] = np.zeros((n_neurons[i + 1], 1))
    else:
        raise ValueError(f"Unknown initialization method: {args.initial_params}")

    return W, b


def forward_propagation(X, W, b):
    """
    Compute the forward propagation of the neural network.
    """
    # The first step will reduce the 784 pixels of each image to 32 neurons;
    # we pass the full training set of 60,000 images through the hidden layer,
    # at once, via the X matrix, transposed.  This is possible because:
    # - X is a matrix of shape 60,000 x 784
    # - W1 is a matrix of shape 32 x 784
    # - b1 is a matrix of shape 32 x 1
    # Therefore by multiplying W1 by the transposition of X, we get a matrix
    # of shape 32 x 60,000, where each column represent the output of the hidden
    # layer (and input for the hidden layer) generated by a single image.

    Z = [None] * n_layers  # list of unactivated outputs
    A = [None] * n_layers  # list of activated outputs
    f = [None] * n_layers  # list of activation functions

    # Initialize the activation functions
    f[0] = lambda x: x  # the input layer output is the input itself
    f[1:-1] = [ReLU] * (n_layers - 2)  # all hidden layers are ReLU activated
    f[-1] = softmax  # the output layer is softmax activated, ideal for classification

    # Forward propagation is an iterative process that starts from the input
    # layer, therefore we need to initialize the input layer manually.
    Z[0] = X
    A[0] = X

    # Forward propagation starts from the input layer and goes
    # all the way to the output layer.
    for i in range(1, n_layers):  # if n_layers = 4, then i = 1, 2, 3 (0 is skipped)
        Z[i] = W[i - 1] @ A[i - 1] + b[i - 1]
        A[i] = f[i](Z[i])

    # This is how it looks like for a network with 1 hidden layer:
    # Z1 = W1 @ X + b1  # 32 x 60,000 unactivated output of the input layer
    # A1 = ReLU(Z1)  # 32 x 60,000 activated output of the input layer
    # Z2 = W2 @ A1 + b2  # 10 x 60,000 unactivated output of the hidden layer
    # A2 = softmax(Z2)  # 10 x 60,000 activated output of the hidden layer

    return Z, A


def back_propagation(X, Y, W, b, Z, A, learning_rate):
    """
    Compute the back propagation of the neural network using thg dataset-averaged
    gradient of the loss function with respect to the weights and biases.
    """

    n_samples = X.shape[1]

    # Initialize the gradients
    δM = [None] * n_layers  # gradient of the loss function wrt the unactivated output
    dL_dW = [None] * n_layers  # gradient of the loss function wrt the weights
    dL_db = [None] * n_layers  # gradient of the loss function wrt the biases

    # Backward propagation is an iterative process that starts from the output
    # layer, therefore we need to initialize the output layer manually. The only
    # variable that we need to initialize is δM[i], which is the gradient of the
    # loss function wrt the unactivated output of the i-th layer.
    # Note: there is no need to persist the δM[i] values, because they are only
    # used to compute the gradients of the weights and biases, and then discarded.
    # We do it here for clarity.
    δM[-1] = A[-1] - Y  # very simple thanks to the cross-entropy loss function
    for i in range(n_layers - 1, 0, -1):  # if n_layers = 4, then i = 3, 2, 1
        δM[i - 1] = W[i - 1].T @ δM[i] * ReLU_derivative(Z[i - 1])

    # Backward propagation starts from the output layer and goes all the way to
    # the input layer.
    for i in range(n_layers - 1, 0, -1):  # if n_layers = 4, then i = 3, 2, 1
        dL_dW[i - 1] = δM[i] @ A[i - 1].T / n_samples
        dL_db[i - 1] = np.sum(δM[i], axis=1, keepdims=True) / n_samples

    # Check the resulting shapes
    for i in range(n_layers - 1):
        if dL_dW[i].shape != W[i].shape:
            raise ValueError(
                f"Gradient dL_dW{i} shape {dL_dW[i].shape} does not match W{i} shape {W[i].shape}"
            )
        if dL_db[i].shape != b[i].shape:
            raise ValueError(
                f"Gradient dL_db{i} shape {dL_db[i].shape} does not match b{i} shape {b[i].shape}"
            )

    # Update the parameters
    for i in range(n_layers - 1):
        W[i] -= learning_rate * dL_dW[i]
        b[i] -= learning_rate * dL_db[i]

    # δM = A2 - Y
    # # ^^^ δM = ∂L/∂x_last has the very simple form A2 - Y thanks to the choice
    # # of the cross-entropy loss function and the softmax activation function,
    # # see the comment above the loss function definition.
    # dL_dW2 = δM @ A1.T / n_samples

    # # Check that the gradient has the expected shape
    # if dL_dW2.shape != W2.shape:
    #     raise ValueError(f"Gradient dL_dW2 shape {dL_dW2.shape} does not match W2 shape {W2.shape}")

    # # The gradient wrt to the bias for a single datapoint is just the δ vector
    # # itself, because ∂L/∂b_i = ∂L/∂x_last * ∂x_last/∂b_i = δ_i * 1 = δ_i.
    # # The average gradient over all cases in the dataset is then:
    # #  - <∂L/∂b_i> = (1/N) * Σₖ δ[i,k]
    # # This is just the sum over the rows of δM, resulting in a m x 1 vector.
    # dL_db2 = np.sum(δM, axis=1, keepdims=True) / n_samples  # keepdims to ensure column vector

    # # Check that the gradient has the expected shape
    # if dL_db2.shape != b2.shape:
    #     raise ValueError(f"Gradient dL_db2 shape {dL_db2.shape} does not match b2 shape {b2.shape}")

    # # The gradient wrt to the weights of the first layer is given by:
    # δM_penultimate = W2.T @ δM * ReLU_derivative(Z1)
    # dL_dW1 = δM_penultimate @ X.T / n_samples

    # # Check that the gradient has the expected shape
    # if dL_dW1.shape != W1.shape:
    #     raise ValueError(f"Gradient dL_dW1 shape {dL_dW1.shape} does not match W1 shape {W1.shape}")

    # # The gradient wrt to the bias for the first layer is given by:
    # dL_db1 = np.sum(δM_penultimate, axis=1, keepdims=True) / n_samples

    # # Check that the gradient has the expected shape
    # if dL_db1.shape != b1.shape:
    #     raise ValueError(f"Gradient dL_db1 shape {dL_db1.shape} does not match b1 shape {b1.shape}")

    # # Update the parameters
    # W2 -= learning_rate * dL_dW2
    # b2 -= learning_rate * dL_db2
    # W1 -= learning_rate * dL_dW1
    # b1 -= learning_rate * dL_db1

    # # Debug: print the magnitude of the gradients compared to the weights
    # # print("Gradient magnitudes:")
    # # print(" - dL_dW2:", np.linalg.norm(dL_dW2) / np.linalg.norm(W2))
    # # print(" - dL_db2:", np.linalg.norm(dL_db2) / np.linalg.norm(b2))
    # # print(" - dL_dW1:", np.linalg.norm(dL_dW1) / np.linalg.norm(W1))
    # # print(" - dL_db1:", np.linalg.norm(dL_db1) / np.linalg.norm(b1))

    return W, b


def print_params_info(W, b):
    for i in range(n_layers - 1):
        print(
            f" - W{i}: {W[i].shape}, min: {np.min(W[i]):.2f}, max: {np.max(W[i]):.2f}, mean: {np.mean(W[i]):.2f}"
        )
        print(
            f" - b{i}: {b[i].shape}, min: {np.min(b[i]):.2f}, max: {np.max(b[i]):.2f}, mean: {np.mean(b[i]):.2f}"
        )


def train(X, Y, W, b, epochs):
    """
    Train the neural network.

    Y must be a one-hot encoded vector.
    """
    for epoch in range(epochs):
        Z, A = forward_propagation(X, W, b)
        W, b = back_propagation(X, Y, W, b, Z, A, learning_rate)
        if epoch % 25 == 0:
            print(f"Epoch {epoch}")
            print(f" - Loss: {np.mean(loss(Y, A[-1]))}")
            Y_pred = get_predictions(A[-1])
            acc = get_accuracy(Y_train, Y_pred)
            print(f" - Accuracy: {acc}")
            print_params_info(W, b)
    return W, b


# Load the MNIST dataset
X_train, Y_train, X_test, Y_test = load_minst_dataset_from_data_folder()

# We shall work with matrices where each row represents a single sample image
X_train = X_train.T
X_test = X_test.T
if X_train.shape != (n_neurons_input, mnist_sample_size):
    raise ValueError(
        f"X_train shape {X_train.shape} does not match expected shape {n_neurons_input, mnist_sample_size}"
    )

print(f"MNIST dataset loaded:")
print(f" - X_train: {X_train.shape}, min: {np.min(X_train)}, max: {np.max(X_train)}")
print(f" - Y_train: {Y_train.shape}, min: {np.min(Y_train)}, max: {np.max(Y_train)}")
print(f" - X_test: {X_test.shape}, min: {np.min(X_test)}, max: {np.max(X_test)}")
print(f" - Y_test: {Y_test.shape}, min: {np.min(Y_test)}, max: {np.max(Y_test)}")
print(f" - one hot encoding of Y_train: {one_hot_encode(Y_train, n_neurons_output).shape}")

# Get initial parameters
W, b = init_params()
print("Parameters initialized:")
print_params_info(W, b)

# Feed the samples to the neural network for the first time
Z, A = forward_propagation(X_train, W, b)
print("Fist forward propagation done:")
for i in range(n_layers):
    print(
        f" - Z[{i}]: {Z[i].shape}, min: {np.min(Z[i])}, max: {np.max(Z[i])}, mean: {np.mean(Z[i])}"
    )
    print(
        f" - A[{i}]: {A[i].shape}, min: {np.min(A[i])}, max: {np.max(A[i])}, mean: {np.mean(A[i])}"
    )

# Get first prediction and visually check the accuracy is 10%
Y_pred = get_predictions(A[-1])
acc = get_accuracy(Y_train, Y_pred)
print("Accuracy without training should be around 10%:", acc)

# Train the neural network
Y_train_one_hot = one_hot_encode(Y_train, n_neurons_output)
W, b = train(X_train, Y_train_one_hot, W, b, epochs=epochs)

# Feed the training samples to the neural network
Z, A = forward_propagation(X_train, W, b)
print("Final accuracy on TRAINING set:")
Y_pred = get_predictions(A[-1])
acc = get_accuracy(Y_train, Y_pred)
print(f" - Accuracy: {acc}")

# Feed the test samples to the neural network
Z, A = forward_propagation(X_test, W, b)
print("Final accuracy on TEST set:")
Y_pred = get_predictions(A[-1])
acc = get_accuracy(Y_test, Y_pred)
print(f" - Accuracy: {acc}")
