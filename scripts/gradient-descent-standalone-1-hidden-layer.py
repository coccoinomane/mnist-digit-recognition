"""
Train a neural-network model for digit recognition on the MNIST dataset,
and get predictions out of it for a test set of 10,000 handwritten digits.

Usage: python -m scripts.gradient-descent-standalone-1-hidden-layer

The script does not make use of any deep-learning framework, as it is a
standalone implementation of a neural network with one hidden layer.

Inspired by Samson Huang great tutorial:
- https://www.youtube.com/watch?v=w8yWXqWQYmU
"""

from src.helpers.mnist import load_minst_dataset_from_data_folder
import numpy as np

# HYPERPARAMETERS
n = 28 * 28  # Number of pixels in each image, also dimension of input layer
n_hidden = 32  # Number of neurons in the hidden layer
n_output = 10  # Number of possible outputs (0-9 digits), same as output layer


def init_params():
    """
    Initialize weights and biases for the neural network.
    """
    W1 = np.random.rand(n_hidden, n) - 0.5  # 32 x 784 numbers between 0 and 1
    b1 = np.random.rand(n_hidden, 1) - 0.5  # 32 x 1 numbers between -0.5 and 0.5
    W2 = np.random.rand(n_output, n_hidden) - 0.5  # 10 x 32 numbers between 0 and 1
    b2 = np.random.rand(n_output, 1) - 0.5  # 32 x 1 numbers between -0.5 and 0.5
    return W1, b1, W2, b2


def ReLU(Z):
    """
    Activation function for the input layer (and for the intermediate
    layers, if any).  Without the activation function, the layers would
    only output linear combinations of the initial input, which would make the
    whole network equivalent to a single layer.
    TODO: What if we use min(0, Z) instead of max(0, Z)?
    """
    return np.maximum(0, Z)


def softmax(Z):
    """
    Function that converts the output of each neuron in the final
    layer into a [0,1] probability.  Z is a 10 x 60,000 matrix, where
    each column represents the output of the final layer for a single image.
    """
    if np.any(Z > 709):
        raise ValueError("Overflow in softmax function")
    exp_Z = np.exp(Z)
    return exp_Z / np.sum(exp_Z, axis=0)


def forwad_propagation(X, W1, b1, W2, b2):
    """
    Compute the forward propagation of the neural network.
    """
    # The first step will reduce the 784 pixels of each image to 32 neurons;
    # we pass the full training set of 60,000 images through the hidden layer,
    # at once, via the X matrix, transposed.  This is possible because:
    # - X is a matrix of shape 60,000 x 784
    # - W1 is a matrix of shape 32 x 784
    # - b1 is a matrix of shape 32 x 1
    # Therefore by multiplying W1 by the transposition of X, we get a matrix
    # of shape 32 x 60,000, where each column represent the output of the hidden
    # layer (and input for the hidden layer) generated by a single image.

    # 32 x 60,000 unactivated output of the input layer. Note how each
    # neuron has its own bias regardless of the input
    Z1 = W1 @ X.T + b1
    A1 = ReLU(Z1)  # 32 x 60,000 activated output of the input layer
    Z2 = W2 @ A1 + b2  # 10 x 60,000 unactivated output of the hidden layer
    A2 = softmax(Z2)  # 10 x 60,000 activated output of the hidden layer
    return Z1, A1, Z2, A2


def get_predictions(A2):
    """
    Get the predictions (Y) out of the activated output of the
    final layer (A2).

    Y is a 60,000 x 1 vector with the predicted digit for
    each image.

    A2 is a 10 x 60,000 matrix, where each column contains the
    probabilities of each digit (0-9) for a single image.
    """
    # argmax returns the indices of the maximum values along an axis.
    return np.argmax(A2, axis=0)


def get_accuracy(Y, Y_pred):
    """
    Compute the accuracy of the predictions.
    """
    return np.mean(Y == Y_pred)


# Load the MNIST dataset
X_train, Y_train, X_test, Y_test = load_minst_dataset_from_data_folder()
print(f"MNIST dataset loaded:")
print(
    f" - X_train: {X_train.shape}, min: {np.min(X_train)}, max: {np.max(X_train)}, mean: {np.mean(X_train)}"
)
print(
    f" - Y_train: {Y_train.shape}, min: {np.min(Y_train)}, max: {np.max(Y_train)}, mean: {np.mean(Y_train)}"
)
print(
    f" - X_test: {X_test.shape}, min: {np.min(X_test)}, max: {np.max(X_test)}, mean: {np.mean(X_test)}"
)
print(
    f" - Y_test: {Y_test.shape}, min: {np.min(Y_test)}, max: {np.max(Y_test)}, mean: {np.mean(Y_test)}"
)

# Get initial parameters
W1, b1, W2, b2 = init_params()
print("Parameters initialized:")
print(f" - W1: {W1.shape}, min: {np.min(W1)}, max: {np.max(W1)}, mean: {np.mean(W1)}")
print(f" - b1: {b1.shape}, min: {np.min(b1)}, max: {np.max(b1)}, mean: {np.mean(b1)}")
print(f" - W2: {W2.shape}, min: {np.min(W2)}, max: {np.max(W2)}, mean: {np.mean(W2)}")
print(f" - b2: {b2.shape}, min: {np.min(b2)}, max: {np.max(b2)}, mean: {np.mean(b2)}")

# Feed the samples to the neural network
Z1, A1, Z2, A2 = forwad_propagation(X_train, W1, b1, W2, b2)
print("Forward propagation done:")
print(f" - Z1: {Z1.shape}, min: {np.min(Z1)}, max: {np.max(Z1)}, mean: {np.mean(Z1)}")
print(f" - A1: {A1.shape}, min: {np.min(A1)}, max: {np.max(A1)}, mean: {np.mean(A1)}")
print(f" - Z2: {Z2.shape}, min: {np.min(Z2)}, max: {np.max(Z2)}, mean: {np.mean(Z2)}")
print(f" - A2: {A2.shape}, min: {np.min(A2)}, max: {np.max(A2)}, mean: {np.mean(A2)}")

# Get predictions
Y_pred = get_predictions(A2)

# Compute accuracy
acc = get_accuracy(Y_train, Y_pred)

print("Accuracy:", acc)
