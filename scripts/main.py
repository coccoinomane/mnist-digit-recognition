"""
Train a neural-network model for digit recognition on the MNIST
dataset with an arbitrary number of hidden layers.

For example, running the following command:

$ python -m scripts.main --n_hidden 32 16 --epochs 150 --learning_rate 0.2

will train a neural network with two hidden layers, the first with 32
neurons, and the second with 16 neurons, for 150 epochs thus achieving
92% precision.

Parameters:
- n_hidden: Number of neurons in each hidden layer, separated by spaces;
    default is "32 16"
- epochs: Number of times the training set is passed through the network
- learning_rate: How much the weights are updated at each iteration
- initial_params: Initialization method for the weights (he, gaussian, samson)
"""

from src.helpers.mnist import load_minst_dataset_from_data_folder
import numpy as np
import argparse
from src.helpers.activations import ReLU, ReLU_derivative, softmax
from src.helpers.loss_functions import cross_entropy_loss as loss
from src.helpers.misc import get_predictions, get_accuracy, one_hot_encode

# PARSE CLI ARGUMENTS
parser = argparse.ArgumentParser(description="Train a neural network on the MNIST dataset.")
parser.add_argument(
    "--n_hidden",
    type=int,
    nargs="+",
    default=[32, 16],
    help="Number of neurons in each hidden layer",
)
parser.add_argument(
    "--epochs",
    type=int,
    default=500,
    help="Number of times the training set is passed through the network",
)
parser.add_argument(
    "--learning_rate",
    type=float,
    default=0.2,
    help="How much the weights are updated at each iteration",
)
parser.add_argument(
    "--initial_params",
    type=str,
    default="he",
    help="Initialization method for the weights",
    choices=["he", "gaussian", "samson"],
)
args = parser.parse_args()

# HYPERPARAMETERS
n_hidden = args.n_hidden
epochs = args.epochs
learning_rate = args.learning_rate

# FIXED PARAMETERS
n_neurons_input = 28 * 28  # Number of pixels in each image, also dimension of input layer
n_neurons_output = 10  # Number of possible outputs (0-9 digits), same as output layer
mnist_sample_size = 60_000  # Number of samples in the MNIST training set

# DYNAMIC PARAMETERS
n_layers = 1 + len(n_hidden) + 1  # Number of layers in the network
n_neurons = [n_neurons_input] + n_hidden + [n_neurons_output]  # Number of neurons in each layer

# Throw error if any hidden layer has non-positive number of neurons
if any(n <= 0 for n in n_hidden):
    raise ValueError("Number of neurons in hidden layers must be positive")

# Print the configuration
print("Configuration:")
print(f" - n hidden layers: {len(n_hidden)}")
print(f" - n neurons in hidden layers: {n_hidden}")
print(f" - n total layers including input and output: {n_layers}")
print(f" - n neurons in all layers: {n_neurons}")
print(f" - epochs: {epochs}")
print(f" - learning rate: {learning_rate}")
print(f" - initial parameters algorithm: {args.initial_params}")


def init_params():
    """
    Initialize weights and biases for the neural network.
    """
    W = [None] * (n_layers - 1)
    b = [None] * (n_layers - 1)

    # Initialize weights and biases:
    # - W[i] connects layer i to layer i+1 and has shape (n_neurons[i+1], n_neurons[i])
    # - b[i] is a column vector with n_neurons[i+1] elements
    if args.initial_params == "samson":
        for i in range(n_layers - 1):
            W[i] = np.random.rand(n_neurons[i + 1], n_neurons[i]) - 0.5
            b[i] = np.random.rand(n_neurons[i + 1], 1) - 0.5
    elif args.initial_params == "he":
        for i in range(n_layers - 1):
            W[i] = np.random.randn(n_neurons[i + 1], n_neurons[i]) * np.sqrt(2.0 / n_neurons[i])
            b[i] = np.zeros((n_neurons[i + 1], 1))
    elif args.initial_params == "gaussian":
        for i in range(n_layers - 1):
            W[i] = np.random.randn(n_neurons[i + 1], n_neurons[i]) * 0.01
            b[i] = np.zeros((n_neurons[i + 1], 1))
    else:
        raise ValueError(f"Unknown initialization method: {args.initial_params}")

    return W, b


def forward_propagation(X, W, b):
    """
    Compute the forward propagation of the neural network.
    """
    # The first step will reduce the 784 pixels of each image to 32 neurons;
    # we pass the full training set of 60,000 images through the hidden layer,
    # at once, via the X matrix, transposed.  This is possible because:
    # - X is a matrix of shape 60,000 x 784
    # - W1 is a matrix of shape 32 x 784
    # - b1 is a matrix of shape 32 x 1
    # Therefore by multiplying W1 by the transposition of X, we get a matrix
    # of shape 32 x 60,000, where each column represent the output of the hidden
    # layer (and input for the hidden layer) generated by a single image.

    Z = [None] * n_layers  # list of unactivated outputs
    A = [None] * n_layers  # list of activated outputs
    f = [None] * n_layers  # list of activation functions

    # Initialize the activation functions
    f[0] = lambda x: x  # the input layer output is the input itself
    f[1:-1] = [ReLU] * (n_layers - 2)  # all hidden layers are ReLU activated
    f[-1] = softmax  # the output layer is softmax activated, ideal for classification

    # Forward propagation is an iterative process that starts from the input
    # layer, therefore we need to initialize the input layer manually.
    Z[0] = X
    A[0] = X

    # Forward propagation starts from the input layer and goes
    # all the way to the output layer.
    for i in range(1, n_layers):  # if n_layers = 4, then i = 1, 2, 3 (0 is skipped)
        Z[i] = W[i - 1] @ A[i - 1] + b[i - 1]
        A[i] = f[i](Z[i])

    # This is how it looks like for a network with 1 hidden layer:
    # Z1 = W1 @ X + b1  # 32 x 60,000 unactivated output of the input layer
    # A1 = ReLU(Z1)  # 32 x 60,000 activated output of the input layer
    # Z2 = W2 @ A1 + b2  # 10 x 60,000 unactivated output of the hidden layer
    # A2 = softmax(Z2)  # 10 x 60,000 activated output of the hidden layer

    return Z, A


def back_propagation(X, Y, W, b, Z, A, learning_rate):
    """
    Compute the back propagation of the neural network using thg dataset-averaged
    gradient of the loss function with respect to the weights and biases.
    """

    n_samples = X.shape[1]

    # Initialize the gradients
    δ = [None] * (n_layers - 1)  # gradient of the loss function wrt the unactivated output
    dL_dW = [None] * (n_layers - 1)  # gradient of the loss function wrt the weights
    dL_db = [None] * (n_layers - 1)  # gradient of the loss function wrt the biases

    # Backward propagation is an iterative process that starts from the output
    # layer.  This means that we need to initialize the values for the output
    # layer manually.
    # The only variable on the output layer that we need to initialize is
    #  δ_last = ∂L/∂Z_last
    # that is, the gradient of the loss function with respect to the input
    # to the output layer (Z_last).
    # Regardles of the number of layers in the network, we have:
    #  δ_last = A_last - Y
    # where A_last is the prediction of the betwork (that is, the activated
    # output of the output layer), and Y is the one-hot encoded target vector.
    # This simple result follow from to the choice of the cross-entropy loss
    # function and the softmax activation function.
    δ = A[-1] - Y
    # For the preceding layers, δ can be computed iteratively:
    #  δ[i-1] = W[i].T @ δ[i] * ReLU_derivative(Z[i])
    # where W[i] is the weight matrix connecting layer i to layer i+1,
    # For example, for a network with 1 hidden layer, we have:
    #  δ[1] = A[1] - Y
    #  δ[0] = W[1].T @ δ[1] * ReLU_derivative(Z[1])
    # For a network with 2 hidden layers, we have:
    #  δ[2] = A[2] - Y
    #  δ[1] = W[2].T @ δ[2] * ReLU_derivative(Z[2])
    #  δ[0] = W[1].T @ δ[1] * ReLU_derivative(Z[1])
    # and so on.

    # Once we know how to compute δ, obtaining the weight gradients is easy:
    #  dL_dW[i] = δ[i] @ A[i].T / n_samples
    # where A[i] is the activated output of layer i, and n_samples is the number
    # of samples in the dataset.  The matrix multiplication δ[i] @ A[i].T takes
    # care of summing the gradients over all samples in the dataset, hence the
    # division by n_samples which returns the average gradient.
    # For the bias gradients, we can use this formula:
    #  dL_db[i] = Σₖ δ[i]ₖ / n_samples
    # where δ[i]ₖ is the k-th element of the δ[i] vector.
    # For example, the gradients of a network with 2 hidden layers is:
    #  dL_dW[2] = δ[2] @ A[2].T / n
    #  dL_db[2] = Σₖ δ[2]ₖ / n
    #  dL_dW[1] = δ[1] @ A[1].T / n
    #  dL_db[1] = Σₖ δ[1]ₖ / n
    #  dL_dW[0] = δ[0] @ A[0].T / n (where A[0] = X)
    #  dL_db[0] = Σₖ δₖ[0] / n
    for i in reversed(range(n_layers - 1)):
        dL_dW[i] = δ @ A[i].T / n_samples
        dL_db[i] = np.sum(δ, axis=1, keepdims=True) / n_samples
        # Compute δ for the lower layer
        if i > 0:  # no need for δ for the input layer
            δ = W[i].T @ δ * ReLU_derivative(Z[i])

    # Check the resulting shapes
    for i in range(n_layers - 1):
        if dL_dW[i].shape != W[i].shape:
            raise ValueError(
                f"Gradient dL_dW{i} shape {dL_dW[i].shape} does not match W{i} shape {W[i].shape}"
            )
        if dL_db[i].shape != b[i].shape:
            raise ValueError(
                f"Gradient dL_db{i} shape {dL_db[i].shape} does not match b{i} shape {b[i].shape}"
            )

    # Update the parameters
    for i in range(n_layers - 1):
        W[i] -= learning_rate * dL_dW[i]
        b[i] -= learning_rate * dL_db[i]

    return W, b


def print_params_info(W, b):
    for i in range(n_layers - 1):
        print(
            f" - W{i}: {W[i].shape}, min: {np.min(W[i]):.2f}, max: {np.max(W[i]):.2f}, mean: {np.mean(W[i]):.2f}"
        )
        print(
            f" - b{i}: {b[i].shape}, min: {np.min(b[i]):.2f}, max: {np.max(b[i]):.2f}, mean: {np.mean(b[i]):.2f}"
        )


def train(X, Y, W, b, epochs):
    """
    Train the neural network.

    Y must be a one-hot encoded vector.
    """
    for epoch in range(epochs):
        Z, A = forward_propagation(X, W, b)
        W, b = back_propagation(X, Y, W, b, Z, A, learning_rate)
        if epoch % 25 == 0:
            print(f"Epoch {epoch}")
            print(f" - Loss: {np.mean(loss(Y, A[-1]))}")
            Y_pred = get_predictions(A[-1])
            acc = get_accuracy(Y_train, Y_pred)
            print(f" - Accuracy: {acc}")
            print_params_info(W, b)
    return W, b


# Load the MNIST dataset
X_train, Y_train, X_test, Y_test = load_minst_dataset_from_data_folder()

# We shall work with matrices where each row represents a single sample image
X_train = X_train.T
X_test = X_test.T
if X_train.shape != (n_neurons_input, mnist_sample_size):
    raise ValueError(
        f"X_train shape {X_train.shape} does not match expected shape {n_neurons_input, mnist_sample_size}"
    )

print(f"MNIST dataset loaded:")
print(f" - X_train: {X_train.shape}, min: {np.min(X_train)}, max: {np.max(X_train)}")
print(f" - Y_train: {Y_train.shape}, min: {np.min(Y_train)}, max: {np.max(Y_train)}")
print(f" - X_test: {X_test.shape}, min: {np.min(X_test)}, max: {np.max(X_test)}")
print(f" - Y_test: {Y_test.shape}, min: {np.min(Y_test)}, max: {np.max(Y_test)}")
print(f" - one hot encoding of Y_train: {one_hot_encode(Y_train, n_neurons_output).shape}")

# Get initial parameters
W, b = init_params()
print("Parameters initialized:")
print_params_info(W, b)

# Feed the samples to the neural network for the first time
Z, A = forward_propagation(X_train, W, b)
print("Fist forward propagation done:")
for i in range(n_layers):
    print(
        f" - Z[{i}]: {Z[i].shape}, min: {np.min(Z[i])}, max: {np.max(Z[i])}, mean: {np.mean(Z[i])}"
    )
    print(
        f" - A[{i}]: {A[i].shape}, min: {np.min(A[i])}, max: {np.max(A[i])}, mean: {np.mean(A[i])}"
    )

# Get first prediction and visually check the accuracy is 10%
Y_pred = get_predictions(A[-1])
acc = get_accuracy(Y_train, Y_pred)
print("Accuracy without training should be around 10%:", acc)

# Train the neural network
Y_train_one_hot = one_hot_encode(Y_train, n_neurons_output)
W, b = train(X_train, Y_train_one_hot, W, b, epochs=epochs)

# Feed the training samples to the neural network
Z, A = forward_propagation(X_train, W, b)
print("Final accuracy on TRAINING set:")
Y_pred = get_predictions(A[-1])
acc = get_accuracy(Y_train, Y_pred)
print(f" - Accuracy: {acc}")

# Feed the test samples to the neural network
Z, A = forward_propagation(X_test, W, b)
print("Final accuracy on TEST set:")
Y_pred = get_predictions(A[-1])
acc = get_accuracy(Y_test, Y_pred)
print(f" - Accuracy: {acc}")
